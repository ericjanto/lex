{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful spacy knowledge\n",
    "\n",
    "- Can use token.is\\_\\* and token.like\\_\\* properties to create a `is_relevant_token(token: Token) -> bool` method\n",
    "- `spacy.explain(\"some label\")` has explanation for most labels\n",
    "- Creating an own doc (could be useful for creating the `context_value` string)\n",
    "\n",
    "```py\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "```\n",
    "\n",
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the nlp.pipe method can speed this up significantly.\n",
    "\n",
    "It processes the texts as a stream and yields Doc objects.\n",
    "\n",
    "It is much faster than just calling nlp on each text, because it batches up the texts.\n",
    "\n",
    "nlp.pipe is a generator that yields Doc objects, so in order to get a list of docs, remember to call the list method around it.\n",
    "\n",
    "```py\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "```\n",
    "\n",
    "A list of [text, context] examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys \"author\" and \"book\".\n",
    "\n",
    "Use the set_extension method to register the custom attributes \"author\" and \"book\" on the Doc, which default to None.\n",
    "Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "Overwrite the doc._.book and doc._.author with the respective info passed in as the context.\n",
    "\n",
    "```py\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]\n",
    "```\n",
    "\n",
    "- Use nlp.make_doc to turn a text into a Doc object, rather than doc. This only tokenises the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dbtypes import LemmaId\n",
    "\n",
    "\n",
    "def get_lemma_id(lemma: str) -> LemmaId:\n",
    "    r = requests.get(f\"{BASE_URL}/lemma/get_lemma_id/{lemma}\")\n",
    "    return LemmaId(r.json()) if r.status_code == 200 else LemmaId(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from dbtypes import Lemma\n",
    "\n",
    "def add_lemma(lemma: Lemma) -> LemmaId:\n",
    "    r = requests.post(\n",
    "        f\"{BASE_URL}/lemma/add_lemma\",\n",
    "        json=lemma.to_dict(),\n",
    "    )\n",
    "    assert r.status_code == 200\n",
    "    assert LemmaId(r.json()) != -1\n",
    "    return LemmaId(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "\n",
    "from dbtypes import StatusId, UposTag\n",
    "\n",
    "context_size_lines = 5\n",
    "parse_into_base_vocab = False\n",
    "\n",
    "prefixes = nlp.Defaults.prefixes + [r\"\"\"^-+\"\"\"]  # type: ignore\n",
    "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search\n",
    "\n",
    "suffixes = nlp.Defaults.suffixes + [\n",
    "    r\"\"\"-+$\"\"\",\n",
    "]  # type: ignore\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "\n",
    "pipelines = [\n",
    "    \"transformer\",\n",
    "    \"tagger\",\n",
    "    \"parser\",\n",
    "    \"attribute_ruler\",\n",
    "    \"lemmatizer\",\n",
    "    \"ner\",\n",
    "]\n",
    "pipeline_combinations = []\n",
    "for r in range(1, len(pipelines) + 1):\n",
    "    pipeline_combinations.extend(list(combinations(pipelines, r)))\n",
    "\n",
    "# default dict with empty list\n",
    "stats = defaultdict(list)\n",
    "\n",
    "with open(\"assets/reference-vocabulary/vocabulary.base.txt\", \"r\") as fvb:\n",
    "    existing_base_vocab = set(fvb.read().split())\n",
    "new_base_vocab = set()\n",
    "\n",
    "with open(\"assets/dev-samples/harry-potter.content.txt\", \"r\") as f:\n",
    "    i = 0\n",
    "    while True:\n",
    "        print(i)\n",
    "        i += 1\n",
    "        context = \" \".join([l.strip() for l in islice(f, context_size_lines)])\n",
    "        if not context or i == 1:\n",
    "            break\n",
    "\n",
    "        # Treat each batch as an own document\n",
    "        # First, tokenise the batch\n",
    "        doc = nlp.make_doc(context)\n",
    "        context_tokens = [(t.text, t.whitespace_) for t in doc]\n",
    "\n",
    "        # Now, lowercase and remove stopwords\n",
    "        processable_tokens = [\n",
    "            t.text.lower() for t in doc if not nlp.vocab[t.text].is_stop\n",
    "        ]\n",
    "\n",
    "        with nlp.select_pipes(\n",
    "            enable=(\n",
    "                \"transformer\",\n",
    "                \"tagger\",\n",
    "                \"attribute_ruler\",\n",
    "                \"lemmatizer\",\n",
    "            )\n",
    "        ):\n",
    "            doc = nlp(context)\n",
    "\n",
    "            relevant_pos = [\n",
    "                UposTag.NOUN.value,\n",
    "                UposTag.VERB.value,\n",
    "                UposTag.ADJ.value,\n",
    "                UposTag.ADV.value,\n",
    "            ]\n",
    "\n",
    "            # If base vocab parsing\n",
    "            if parse_into_base_vocab:\n",
    "                for token in doc:\n",
    "                    if (\n",
    "                        bool((lemma := token.lemma_.lower()))\n",
    "                        and not token.is_stop\n",
    "                        and not token.like_num\n",
    "                        and not token.is_space\n",
    "                        and not token.is_digit\n",
    "                        and token.is_alpha\n",
    "                        and token.pos_ in relevant_pos\n",
    "                        and lemma not in existing_base_vocab\n",
    "                    ):\n",
    "                        new_base_vocab.add(lemma)\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                with open(\n",
    "                    \"assets/reference-vocabulary/vocabulary.base.txt\", \"r\"\n",
    "                ) as fvb:\n",
    "                    base_vocab = set(fvb.read().split())\n",
    "                    with open(\n",
    "                        \"assets/reference-vocabulary/vocabulary.irrelevant.txt\",\n",
    "                        \"r\",\n",
    "                    ) as fvi:\n",
    "                        irrelevant_vocab = set(fvi.read().split())\n",
    "                        doc = list(\n",
    "                            filter(\n",
    "                                lambda t: not t.is_stop\n",
    "                                and not t.like_num\n",
    "                                and not t.is_space\n",
    "                                and not t.is_digit\n",
    "                                and t.is_alpha\n",
    "                                and t.pos_ in relevant_pos\n",
    "                                and (l := t.lemma_.lower()) not in base_vocab\n",
    "                                and l not in irrelevant_vocab,\n",
    "                                doc,\n",
    "                            )\n",
    "                        )\n",
    "                # (token, lemma, tag, pos)\n",
    "                db_data = [\n",
    "                    (\n",
    "                        token.text,\n",
    "                        token.lemma_.lower(),\n",
    "                        token.tag_,\n",
    "                        token.pos_,\n",
    "                    )\n",
    "                    for token in doc\n",
    "                ]\n",
    "\n",
    "                # [API needs]: POST (STATUS.PENDING) -> status_id\n",
    "                # TODO\n",
    "\n",
    "                # [API needs]: POST (lemma, status) -> lemma_id\n",
    "                token_lemmaId_map = {\n",
    "                    token: add_lemma(Lemma(lemma=lemma, status_id=StatusId(1)))\n",
    "                    for token, lemma, _, _ in db_data\n",
    "                }\n",
    "\n",
    "                # with nlp.select_pipes(enable=('transformer', 'tagger', 'attribute_ruler', 'lemmatizer')):\n",
    "                #     lemmatized_doc = nlp(context)\n",
    "                #         for t in lemmatized_doc:\n",
    "                #             stats.append(t.lemma_.lower() == t.text.lower())\n",
    "                #         print(f\"Unchanged: {sum(stats)} / {len(stats)}\")\n",
    "                #     if i == 50:\n",
    "                #         break\n",
    "\n",
    "                # for each possible combination of pipelines, run the doc through the pipeline\n",
    "                # and check whether the lemma is the same as the text. Save the results in a\n",
    "                # dictionary with the pipeline combination as key and the number of unchanged\n",
    "                # lemmas as value\n",
    "                # if i < 50:\n",
    "                #     for c in pipeline_combinations:\n",
    "                #         if \"lemmatizer\" not in c:\n",
    "                #             continue\n",
    "                #         with nlp.select_pipes(enable=c):\n",
    "                #             lemmatized_doc = nlp(context)\n",
    "                #             stats[c].append(\n",
    "                #                 sum(\n",
    "                #                     t.lemma_.lower() == t.text.lower()\n",
    "                #                     for t in lemmatized_doc\n",
    "                #                 )\n",
    "                #                 / len(lemmatized_doc)\n",
    "                #             )\n",
    "\n",
    "            # print(doc)\n",
    "            # print([w.whitespace_ for w in doc])\n",
    "    # Write new base vocab in batches\n",
    "    with open(\"assets/reference-vocabulary/vocabulary.base.txt\", \"a\") as fvb:\n",
    "        for word in new_base_vocab:\n",
    "            fvb.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest = 1\n",
    "# lowest_k = \"\"\n",
    "# for k, v in stats.items():\n",
    "#     if sum(v) / len(v) < lowest:\n",
    "#         lowest = sum(v) / len(v)\n",
    "#         lowest_k = k\n",
    "# print(lowest_k, lowest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
